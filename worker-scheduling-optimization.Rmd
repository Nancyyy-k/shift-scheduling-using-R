---
title: “Workforce Scheduling Optimization Using Integer Programming”
output: html_document
date: "2025-04-13"
---
# Nancy kallampet

```{r}
library(dplyr)
library(cluster)
library(ggplot2)
library(dendextend)
library(factoextra)
library(dbscan)
```

```{r}
wines_data <- read.csv("C:\\Users\\nkall\\Downloads\\Wine-clustering-HC.csv")
```

```{r}
head(wines_data)
```

#Remove categorical variables and select only numeric columns
```{r}
wines_numeric <- wines_data %>% 
  select(Alcohol, Malic_Acid, Ash, Ash_Alcanity, Magnesium, Total_Phenols, 
         Flavanoids, Nonflavanoid_Phenols, Proanthocyanins, Color_Intensity, 
         Hue, OD280, Proline)
```

#Normalise the data
```{r}
wine_scaled <- scale(wines_numeric)
```

#AGNES CLUSTERING WITH DIFFERENT LINKAGES
#Different linkage methods
```{r}
agnes_single <- agnes(wine_scaled, method = "single")
agnes_complete <- agnes(wine_scaled, method = "complete")
agnes_average <- agnes(wine_scaled, method = "average")
agnes_ward <- agnes(wine_scaled, method = "ward")
```

```{r}
print(agnes_single$ac)
print(agnes_complete$ac)
print(agnes_average$ac)
print(agnes_ward$ac)
```

#Plot dendograms
```{r}
pltree(agnes_single, main = "AGNES - Single Linkage", hang = -1)
```

```{r}
pltree(agnes_complete, main = "AGNES - Complete Linkage", hang = -1)
```

```{r}
pltree(agnes_average, main = "AGNES - Average Linkage", hang = -1)
```

```{r}
pltree(agnes_ward, main = "AGNES - Ward Linkage", hang = -1)
rect.hclust(as.hclust(agnes_ward), k = 3, border = 2:4)
```

#Diana clustering
```{r}
diana_model_result <- diana(wine_scaled)
print(diana_model_result$ac)
```

```{r}
pltree(diana_model_result,  main = "DIANA Clustering", hang = -1)
rect.hclust(as.hclust(diana_model_result), k = 3, border = 2:4)
```





#QUESTION 1(B)
```{r}
print(agnes_single$ac)
print(agnes_complete$ac)
print(agnes_average$ac)
print(agnes_ward$ac)
print(diana_model_result$ac)
```

#Comparision of AC AGNES and DC of DIANA

The Agglomerative Clustering(AC) using AGNES yielded the following coefficients for different linkage methods:

Single: 0.5379
Complete: 0.8159
Average: 0.7007
Ward: 0.9419
The highest cohesion was achieved under Ward linkage, which suggests that it creates the best-shapen clusters.However, DIANA returned a NULL divisive coefficient(DC), which means that it was unable to compute a valid measure of cluster strength — perhaps due to data characteristics or inner calculation limitations.Thus, based on the coefficients provided, AGNES with Ward linkage provides us with a denser clustering structure, as suggested by its high Agglomerative Coefficient of 0.9419.


#Question 2(A)
CALUCULATING MEDIAN BY GROUPS 
#1.Alcohol_Level
```{r}
median_alcohol <- wines_data %>%
  group_by(Alcohol_Level) %>%
  summarise(Median_Alcohol = median(Alcohol, na.rm = TRUE))

print(median_alcohol)
```

#2. Ash_content
```{r}
median_ash <- wines_data %>%
  group_by(Ash_Content) %>%
  summarise(Median_Ash = median(Ash, na.rm = TRUE))

print(median_ash)
```

#3.Color_Intensity_Group
```{r}
median_color_intensity <- wines_data %>%
  group_by(Color_Intensity_Group) %>%
  summarise(Median_Color_Intensity = median(Color_Intensity, na.rm = TRUE))

print(median_color_intensity)
```



#QUESTION 2(B)
```{r}
ggplot(median_alcohol, aes(x = Alcohol_Level, y = Median_Alcohol, fill = Alcohol_Level)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Median Alcohol by Alcohol Level",
    x = "Alcohol Level",
    y = "Median Alcohol"
  ) +
  theme_minimal()
```

#Plot For Ash_Content
```{r}
ggplot(median_ash, aes(x = Ash_Content, y = Median_Ash, fill = Ash_Content)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Median Ash by Ash Content",
    x = "Ash Content",
    y = "Median Ash"
  ) +
  theme_minimal()
```

# Plot for Color_Intensity_Group
```{r}
ggplot(median_color_intensity, aes(x = Color_Intensity_Group, y = Median_Color_Intensity, fill = Color_Intensity_Group)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Median Color Intensity by Color Intensity Group",
    x = "Color Intensity Group",
    y = "Median Color Intensity"
  ) +
  theme_minimal()
```

#QUESTION 2(C)
#Interpretation of the Bar Plots

The bar plots are graphically depicting the median values of three quantitative variables (Alcohol, Ash, and Color_Intensity) for their respective qualitative groupings (Alcohol_Level, Ash_Content, and Color_Intensity_Group). These plots allow us to comprehend whether the qualitative groupings effectively represent differences in the original numeric data.

#1.Alcohol Level VS Median Alcohol Value
The bar plot of median Alcohol by different Alcohol_Level categories very clearly shows a trend: as the Alcohol_Level increases, the median alcohol also increases.It shows that the separation into Alcohol_Level agrees with the actual Alcohol values of the data — wines separated by higher Alcohol_Level do have higher percentages of alcohol. It also implies that there is a large separation among the levels, something that would be valuable in clustering or classification.


#2.Ash Content VS Median Ash Value

For the Ash_Content plot within the Ash_Content groups, median Ash varies. For instance, if "High" Ash_Content group has considerably greater median Ash than "Low" or "Medium," then it is an indicator of efficiency of this grouping.But if they are minute, it may be an indication that Ash_Content classification can be enhanced or that Ash is more dispersed. Generally, it indicates whether Ash plays an effective role in wine differentiation.


#3.Color Intensity Group VS Median Color Intensity

(Medium → High).
This implies that the grouping aligns with real color intensity levels. Wines in the "High" category are indeed darker or more colored. This feature can be linked to other features like wine age, grape variety, or processing style.


#CONCLUSION

All three categorical features show that their groupings are partially significant in capturing trends among the numeric data.
Alcohol_Level and Color_Intensity_Group are well in sync with their numeric counterparts — and so are also highly reliable features.Ash_Content, while still very useful, shows less sharp separation, and may have to be reprocessed or may be of lesser utility towards making the model accurate.


# QUESTION3(A)
#Appropriate clustering techniques

AGNES algorithm builds clusters bottom-up systematically by merging the closest pairs step-by-step until a hierarchy is complete. Among linkage methods tried (Single, Complete, Average, and Ward), Ward linkage produced the highest Agglomerative Coefficient (AC = 0.9419). This indicates very strong internal cohesion and well-defined cluster development.

Still, DIANA, with a top-down (divisive) approach, failed to provide a proper Divisive Coefficient (DC) — the answer was NULL. This suggests that either the structure of the dataset did not correspond to DIANA's splitting criterion, or the inherent similarity between instances had prevented DIANA from meaningfully assessing separation.

Conclusion : AGNES,with Ward linkage is more suitable for this dataset due to its quantifiable clustering quality and clearly distinct clusters.

#Comparing AGNES and DIANA for Wine Dataset Analysis
AGNES (Ward linkage):
The AGNES dendrogram with Ward linkage strongly suggested the formation of three well-defined clusters, evidenced by sudden vertical cuts and tight clustering in branches.The extremely high Agglomerative Coefficient (0.9419) quantitatively confirms that clusters are cohesive — points within each cluster are very similar to each other, but not similar to points in other clusters.The tree structure showed that the merging choices were made with minimal distortion and resulted in natural partitions of the data.

DIANA:
While the DIANA dendrogram also appeared to have a three-cluster structure, it lacked clear boundaries, and the clusters were not as compact as in AGNES.Interestingly, the Divisive Coefficient was NULL, indicating that the algorithm was unable to quantify the strength of clustering, thus undermining its credibility.
Being a divisive algorithm, DIANA starts with all observations in one cluster and recursively splits them. This sometimes results in premature or inferior splits, especially if there are latent patterns in the data requiring finer bottom-up clustering (as provided by AGNES).

Conclusion: AGNES gives more consistent and clear-cut separation of clusters visually and numerically and is thus better suited to gaining insight into the structure of the wine dataset.

#Patterns and Insights
The clusters which were derived, particularly with Ward linkage and AGNES, were surprisingly found to contain intriguing groupings congruent with real wine attributes. When the cluster memberships were compared to the categorical variables (Color_Intensity_Group, Ash_Content, Alcohol_Level), some intriguing patterns emerged:

High Alcohol_Level and high Color_Intensity_Group wines are one of the groups, i.e., wines with high alcohol content and intense color will be full-bodied intense wines, may be red wines with increased grape skin contact while fermenting. These wines may be favored by consumers for such strong and intense flavor, and may also point towards high-quality or aged types of wines.

The second group matches wines with low to medium Alcohol_Level and lower Color_Intensity, potentially lighter-bodied or white wines with less assertive profiles and potentially other aging techniques or grape varieties.The second cluster had medium levels of Ash and Alcohol, possibly a group of well-balanced table wines. They are not radical in any one chemical attribute but are constant in composition, possibly fit for regular consumption or blending.

Aside from that, the clustering pattern was quite in line with Q2's median values, supporting the notion that alcoholic factors such as ash, alcohol, and color can successfully identify wine variety.

The clusters are pragmatically communicating to winemaker or sommelier viewpoint when highlighting natural affinities in a wine cluster, which would most probably be standing for types or methods of winemaking or wine.From a machine learning point of view, the clusters would be useful in targeted marketing, wine recommendation systems, quality control, or outlier detection.

The results indicate that clustering is an effective unsupervised learning technique for the detection of hidden structure in complex multivariate data sets such as wine profiles.

Conclusion :AGNES with Ward linkage clearly is the better hierarchical clustering procedure for these data.It provides stronger cohesion, better separation, and more meaningful clusters.The clusters reveal well-defined wine profiles that are interesting both from a domain (wine industry) and data science perspective.


# QUESTION 3(B)

# K- MEANS 
```{r}
wine_numeric <- wines_data[sapply(wines_data, is.numeric)]


# Scale the numeric data
wines_scaled <- scale(wine_numeric)

# Determine Optimal Number of Clusters (Elbow Method)
fviz_nbclust(wines_scaled, kmeans, method = "wss") + 
  labs(title = "Elbow Method for Optimal K")
```

#Apply K-Means Clustering (e.g., k = 3)
```{r}
set.seed(123)  # for reproducibility
kmeans_result <- kmeans(wines_scaled, centers = 3, nstart = 25)

#add cluster labels to original data
wines_data$KMeans_Cluster <- as.factor(kmeans_result$cluster)

#Visualise clusters 
fviz_cluster(kmeans_result, data = wines_scaled,
             geom = "point", ellipse.type = "norm",
             main = "K-Means Clustering Result")

```


# DBSCAN CLUSTERING:

#Find optimal eps using knn plot
```{r}
kNNdistplot(wine_scaled, k = 5)
abline(h = 2, lty = 2)
```

```{r}
dbscan_result <- dbscan(wines_scaled, eps = 2, minPts = 5)

# Add cluster labels to the dataset
wines_data$DBSCAN_Cluster <- as.factor(dbscan_result$cluster)

# Visualize DBSCAN result (PCA view)
fviz_cluster(list(data = wines_scaled, cluster = dbscan_result$cluster),
             geom = "point", ellipse.type = "norm",
             main = "DBSCAN Clustering Result")
```

#INTERPRETATION:
K-Means clustering is a suitable method in which clusters are roughly spherical in shape and of comparable size, and the number of clusters k is either known or can be approximated well, say by employing the use of the elbow method. In this case, the dataset appears to have three distinct clusters, and let k=3. Furthermore, since the dataset is large, the efficiency of K-Means is called into action. The data has been scaled using the scale() function, as appropriate given that K-Means operates on Euclidean distance, and scaling will ensure all features have an equal impact. Notably, AGNES using Ward's method also discovered well-separated and tight clusters, which suggests that K-Means can potentially discover similar structure. However, it has to be considered that K-Means is sensitive to centroids' initial placement and that it can end at a local maximum. Furthermore, it also expects equality in clusters' size and density, something that may not always occur when dealing with data in the real world like that of the wine dataset.

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a robust clustering algorithm that is designed to identify clusters of any shape, handle outliers or noise appropriately, and cluster without specifying the number of clusters beforehand. However, in the wine dataset, DBSCAN may not be the most suitable choice. The AGNES dendrogram also shows that clusters in this data are fairly regular and dense, suggesting a spherical and more compact structure. The data is not either noisy or containing outliers as well, under which conditions DBSCAN would actually perform best. The second restriction is the vulnerability of DBSCAN to its own parameters, particularly epsilon (eps) and minPts, and it can sometimes be difficult to adjust them well, particularly on scaled high-dimensional data. Due to the same shape and minimal noise of the clusters, DBSCAN is likely less appropriate for this provided dataset.

#Conclusion
Although each of the three methods has different strengths, AGNES with Ward linkage remains the most appropriate clustering method for the wine data. It is the most desirable combination of interpretability, cohesion, and clarity of cluster structure. 

K-Means offers an appealing alternative whenever computational efficiency or information about the number of clusters is needed. It performs very much like AGNES in terms of the treatment of cluster shape and cluster separation.

DBSCAN, although efficient in noisy or complex datasets, is not optimally effective for this clean and dense dataset and therefore is not the most suitable here.

















 





















